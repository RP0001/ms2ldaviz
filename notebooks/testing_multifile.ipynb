{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import time\n",
    "\n",
    "import sys\n",
    "datadir = '/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/'\n",
    "basedir = '/Users/joewandy/git/lda/code/'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from multifile_feature import SparseFeatureExtractor\n",
    "from lda import VariationalLDA,MultiFileVariationalLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                     loss_threshold_min_count, loss_threshold_max_val):\n",
    "    \n",
    "    extractor = SparseFeatureExtractor(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                                       loss_threshold_min_count, loss_threshold_max_val,\n",
    "                                       input_type='filename')\n",
    "    \n",
    "    # create the grouping for the fragments\n",
    "    fragment_q = extractor.make_fragment_queue()\n",
    "    fragment_groups = extractor.group_features(fragment_q, extractor.fragment_grouping_tol)\n",
    "    \n",
    "    # create the grouping for the losses\n",
    "    loss_q = extractor.make_loss_queue()\n",
    "    loss_groups = extractor.group_features(loss_q, extractor.loss_grouping_tol, \n",
    "                                           check_threshold=True)\n",
    "    \n",
    "    # populate the counts\n",
    "    extractor.create_counts(fragment_groups, loss_groups, scaling_factor)\n",
    "    \n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaling_factor=1000             # previously set to 100 in the single file LDA \n",
    "fragment_grouping_tol=7        # grouping tolerance in ppm for the fragment\n",
    "loss_grouping_tol=7            # grouping tolerance in ppm for the neutral loss\n",
    "loss_threshold_min_count=5     # min. counts of loss values to occur\n",
    "loss_threshold_max_val=200     # max. loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_set = [\n",
    "    (datadir + 'Urine_StrokeDrugs_02_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_02_T10_POS_ms2.csv'),\n",
    "    (datadir + 'Urine_StrokeDrugs_03_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_03_T10_POS_ms2.csv'),\n",
    "    (datadir + 'Urine_StrokeDrugs_08_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_08_T10_POS_ms2.csv'),\n",
    "    (datadir + 'Urine_StrokeDrugs_09_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_09_T10_POS_ms2.csv'),\n",
    "    (datadir + 'Urine_StrokeDrugs_17_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_17_T10_POS_ms2.csv'),\n",
    "    (datadir + 'Urine_StrokeDrugs_18_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_18_T10_POS_ms2.csv'),\n",
    "    (datadir + 'Urine_StrokeDrugs_28_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_28_T10_POS_ms2.csv'),\n",
    "    (datadir + 'Urine_StrokeDrugs_32_T10_POS_ms1.csv', datadir + 'Urine_StrokeDrugs_32_T10_POS_ms2.csv'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_02_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_02_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_03_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_03_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_08_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_08_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_09_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_09_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_17_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_17_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_18_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_18_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_28_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_28_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_32_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_32_T10_POS_ms2.csv\n",
      "Processing fragments for file 0\n",
      "Processing fragments for file 1\n",
      "Processing fragments for file 2\n",
      "Processing fragments for file 3\n",
      "Processing fragments for file 4\n",
      "Processing fragments for file 5\n",
      "Processing fragments for file 6\n",
      "Processing fragments for file 7\n",
      "Total groups=3088\n",
      "Processing losses for file 0\n",
      "Processing losses for file 1\n",
      "Processing losses for file 2\n",
      "Processing losses for file 3\n",
      "Processing losses for file 4\n",
      "Processing losses for file 5\n",
      "Processing losses for file 6\n",
      "Processing losses for file 7\n",
      "Total groups=1220\n",
      "Initialising sparse matrix 0\n",
      "Initialising sparse matrix 1\n",
      "Initialising sparse matrix 2\n",
      "Initialising sparse matrix 3\n",
      "Initialising sparse matrix 4\n",
      "Initialising sparse matrix 5\n",
      "Initialising sparse matrix 6\n",
      "Initialising sparse matrix 7\n",
      "Populating the counts\n",
      "Populating counts for fragment group 0/3088\n",
      "Populating counts for fragment group 100/3088\n",
      "Populating counts for fragment group 200/3088\n",
      "Populating counts for fragment group 300/3088\n",
      "Populating counts for fragment group 400/3088\n",
      "Populating counts for fragment group 500/3088\n",
      "Populating counts for fragment group 600/3088\n",
      "Populating counts for fragment group 700/3088\n",
      "Populating counts for fragment group 800/3088\n",
      "Populating counts for fragment group 900/3088\n",
      "Populating counts for fragment group 1000/3088\n",
      "Populating counts for fragment group 1100/3088\n",
      "Populating counts for fragment group 1200/3088\n",
      "Populating counts for fragment group 1300/3088\n",
      "Populating counts for fragment group 1400/3088\n",
      "Populating counts for fragment group 1500/3088\n",
      "Populating counts for fragment group 1600/3088\n",
      "Populating counts for fragment group 1700/3088\n",
      "Populating counts for fragment group 1800/3088\n",
      "Populating counts for fragment group 1900/3088\n",
      "Populating counts for fragment group 2000/3088\n",
      "Populating counts for fragment group 2100/3088\n",
      "Populating counts for fragment group 2200/3088\n",
      "Populating counts for fragment group 2300/3088\n",
      "Populating counts for fragment group 2400/3088\n",
      "Populating counts for fragment group 2500/3088\n",
      "Populating counts for fragment group 2600/3088\n",
      "Populating counts for fragment group 2700/3088\n",
      "Populating counts for fragment group 2800/3088\n",
      "Populating counts for fragment group 2900/3088\n",
      "Populating counts for fragment group 3000/3088\n",
      "Populating counts for loss group 0/1220\n",
      "Populating counts for loss group 100/1220\n",
      "Populating counts for loss group 200/1220\n",
      "Populating counts for loss group 300/1220\n",
      "Populating counts for loss group 400/1220\n",
      "Populating counts for loss group 500/1220\n",
      "Populating counts for loss group 600/1220\n",
      "Populating counts for loss group 700/1220\n",
      "Populating counts for loss group 800/1220\n",
      "Populating counts for loss group 900/1220\n",
      "Populating counts for loss group 1000/1220\n",
      "Populating counts for loss group 1100/1220\n",
      "Populating counts for loss group 1200/1220\n",
      "Normalising dataframe 0\n",
      "file 0 normalising\n",
      "file 0 normalised csc shape (818, 4308)\n",
      "Normalising dataframe 1\n",
      "file 1 normalising\n",
      "file 1 normalised csc shape (685, 4308)\n",
      "Normalising dataframe 2\n",
      "file 2 normalising\n",
      "file 2 normalised csc shape (740, 4308)\n",
      "Normalising dataframe 3\n",
      "file 3 normalising\n",
      "file 3 normalised csc shape (533, 4308)\n",
      "Normalising dataframe 4\n",
      "file 4 normalising\n",
      "file 4 normalised csc shape (731, 4308)\n",
      "Normalising dataframe 5\n",
      "file 5 normalising\n",
      "file 5 normalised csc shape (616, 4308)\n",
      "Normalising dataframe 6\n",
      "file 6 normalising\n",
      "file 6 normalised csc shape (734, 4308)\n",
      "Normalising dataframe 7\n",
      "file 7 normalising\n",
      "file 7 normalised csc shape (429, 4308)\n"
     ]
    }
   ],
   "source": [
    "extractor = extract_features(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                            loss_threshold_min_count, loss_threshold_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (818, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 818\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 8844\n",
      "\n",
      "File 1\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (685, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 685\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6888\n",
      "\n",
      "File 2\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (740, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 740\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 7898\n",
      "\n",
      "File 3\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (533, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 533\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 4402\n",
      "\n",
      "File 4\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (731, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 731\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6271\n",
      "\n",
      "File 5\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (616, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 616\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 5685\n",
      "\n",
      "File 6\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (734, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 734\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6384\n",
      "\n",
      "File 7\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (429, 4308)\n",
      "Vocab 4308 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 429\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 3618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in range(extractor.F):\n",
    "    mat, vocab, ms1, ms2 = extractor.get_entry(f)\n",
    "    print 'File %d' % f\n",
    "    print 'Count matrix', type(mat), mat.shape\n",
    "    print 'Vocab', len(vocab), 'words'\n",
    "    print 'MS1 rows', type(ms1), ms1.shape[0]\n",
    "    print 'MS2 rows', type(ms2), ms2.shape[0]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_word_index = {}\n",
    "for i,v in enumerate(vocab):\n",
    "    global_word_index[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0\n",
      "818 4308\n",
      "Processing file 1\n",
      "685 4308\n",
      "Processing file 2\n",
      "740 4308\n",
      "Processing file 3\n",
      "533 4308\n",
      "Processing file 4\n",
      "731 4308\n",
      "Processing file 5\n",
      "616 4308\n",
      "Processing file 6\n",
      "734 4308\n",
      "Processing file 7\n",
      "429 4308\n"
     ]
    }
   ],
   "source": [
    "corpus_dictionary = {}\n",
    "for f in range(extractor.F):\n",
    "    print \"Processing file {}\".format(f)\n",
    "    corpus = {}\n",
    "    mat, vocab, ms1, ms2 = extractor.get_entry(f)\n",
    "    n_docs,n_words = mat.shape\n",
    "    print n_docs,n_words\n",
    "    d_pos = 0\n",
    "    for d in ms1.iterrows():\n",
    "        doc_name = \"{}_{}\".format(d[1]['mz'],d[1]['rt'])\n",
    "        corpus[doc_name] = {}\n",
    "        for word_index,count in zip(mat[d_pos,:].rows[0],mat[d_pos,:].data[0]):\n",
    "            if count > 0:\n",
    "                corpus[doc_name][vocab[word_index]] = count\n",
    "        d_pos += 1\n",
    "        \n",
    "    # Added by Simon\n",
    "    name = input_set[f][0].split('/')[-1].split('ms1')[0][:-1]\n",
    "    corpus_dictionary[name] = corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial, update_alpha = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object created with 616 documents\n",
      "Object created with 533 documents\n",
      "Object created with 685 documents\n",
      "Object created with 740 documents\n",
      "Object created with 818 documents\n",
      "Object created with 731 documents\n",
      "Object created with 429 documents\n",
      "Object created with 734 documents\n",
      "serial processing\n",
      "Iteration: 0\n",
      "431.076461286\n",
      "Iteration: 1\n",
      "18.0705085689\n",
      "Iteration: 2\n",
      "17.859806028\n",
      "Iteration: 3\n",
      "17.2158991154"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "mf_lda = MultiFileVariationalLDA(corpus_dictionary,word_index = global_word_index,K = 300,\n",
    "                                 alpha=1,eta=0.1,update_alpha=True)\n",
    "mf_lda.run_vb(parallel=False, n_its=100, initialise=True)\n",
    "end_time = time.clock()\n",
    "total_time_serial = end_time - start_time\n",
    "print 'total time', total_time_serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralle, update_alpha = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "mf_lda = MultiFileVariationalLDA(corpus_dictionary,word_index = global_word_index,K = 300,\n",
    "                                 alpha=1,eta=0.1,update_alpha=True)\n",
    "mf_lda.run_vb(parallel=True, n_its=100, initialise=True)\n",
    "end_time = time.clock()\n",
    "total_time_parallel = end_time - start_time\n",
    "print 'total time', total_time_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timing = [total_time_serial, total_time_parallel]\n",
    "xs = np.arange(len(timing))\n",
    "labels = ['serial', 'parallel']\n",
    "plt.bar(xs, timing)\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xticks(xs+0.4, labels)\n",
    "plt.title('VB --- 8 files 100 steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
