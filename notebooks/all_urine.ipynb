{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import sys\n",
    "basedir = '/Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "sys.path.append('/Users/simon/git/lda/code/')\n",
    "\n",
    "from multifile_feature import SparseFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                     loss_threshold_min_count, loss_threshold_max_val):\n",
    "    \n",
    "    extractor = SparseFeatureExtractor(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                                       loss_threshold_min_count, loss_threshold_max_val,\n",
    "                                       input_type='filename')\n",
    "    \n",
    "    # create the grouping for the fragments\n",
    "    fragment_q = extractor.make_fragment_queue()\n",
    "    fragment_groups = extractor.group_features(fragment_q, extractor.fragment_grouping_tol)\n",
    "    \n",
    "    # create the grouping for the losses\n",
    "    loss_q = extractor.make_loss_queue()\n",
    "    loss_groups = extractor.group_features(loss_q, extractor.loss_grouping_tol, \n",
    "                                           check_threshold=True)\n",
    "    \n",
    "    # populate the counts\n",
    "    extractor.create_counts(fragment_groups, loss_groups, scaling_factor)\n",
    "    \n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaling_factor=100             # previously set to 100 in the single file LDA \n",
    "fragment_grouping_tol=7        # grouping tolerance in ppm for the fragment\n",
    "loss_grouping_tol=7            # grouping tolerance in ppm for the neutral loss\n",
    "loss_threshold_min_count=5     # min. counts of loss values to occur\n",
    "loss_threshold_max_val=200     # max. loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_set = [\n",
    "    (basedir + 'Pooled_Urine_pos_Method1_ms1.csv', basedir + 'Pooled_Urine_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine5_pos_Method1_ms1.csv', basedir + 'Urine5_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine19_pos_Method1_ms1.csv', basedir + 'Urine19_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine20_pos_Method1_ms1.csv', basedir + 'Urine20_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine37_pos_Method1_ms1.csv', basedir + 'Urine37_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine44_pos_Method1_ms1.csv', basedir + 'Urine44_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine61_pos_Method1_ms1.csv', basedir + 'Urine61_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine64_pos_Method1_ms1.csv', basedir + 'Urine64_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine73_pos_Method1_ms1.csv', basedir + 'Urine73_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine74_pos_Method1_ms1.csv', basedir + 'Urine74_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine87_pos_Method1_ms1.csv', basedir + 'Urine87_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine90_pos_Method1_ms1.csv', basedir + 'Urine90_pos_Method1_ms2.csv'),\n",
    "    (basedir + 'Urine91_pos_Method1_ms1.csv', basedir + 'Urine91_pos_Method1_ms2.csv'),\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Pooled_Urine_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Pooled_Urine_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine5_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine5_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine19_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine19_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine20_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine20_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine37_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine37_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine44_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine44_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine61_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine61_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine64_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine64_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine73_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine73_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine74_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine74_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine87_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine87_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine90_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine90_pos_Method1_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine91_pos_Method1_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/input/urine/13_files_separate_mode_method1/POS/Urine91_pos_Method1_ms2.csv\n",
      "Processing fragments for file 0\n",
      "Processing fragments for file 1\n",
      "Processing fragments for file 2\n",
      "Processing fragments for file 3\n",
      "Processing fragments for file 4\n",
      "Processing fragments for file 5\n",
      "Processing fragments for file 6\n",
      "Processing fragments for file 7\n",
      "Processing fragments for file 8\n",
      "Processing fragments for file 9\n",
      "Processing fragments for file 10\n",
      "Processing fragments for file 11\n",
      "Processing fragments for file 12\n",
      "Total groups=4084\n",
      "Processing losses for file 0\n",
      "Processing losses for file 1\n",
      "Processing losses for file 2\n",
      "Processing losses for file 3\n",
      "Processing losses for file 4\n",
      "Processing losses for file 5\n",
      "Processing losses for file 6\n",
      "Processing losses for file 7\n",
      "Processing losses for file 8\n",
      "Processing losses for file 9\n",
      "Processing losses for file 10\n",
      "Processing losses for file 11\n",
      "Processing losses for file 12\n",
      "Total groups=1682\n",
      "Initialising sparse matrix 0\n",
      "Initialising sparse matrix 1\n",
      "Initialising sparse matrix 2\n",
      "Initialising sparse matrix 3\n",
      "Initialising sparse matrix 4\n",
      "Initialising sparse matrix 5\n",
      "Initialising sparse matrix 6\n",
      "Initialising sparse matrix 7\n",
      "Initialising sparse matrix 8\n",
      "Initialising sparse matrix 9\n",
      "Initialising sparse matrix 10\n",
      "Initialising sparse matrix 11\n",
      "Initialising sparse matrix 12\n",
      "Populating the counts\n",
      "Populating counts for fragment group 0/4084\n",
      "Populating counts for fragment group 100/4084\n",
      "Populating counts for fragment group 200/4084\n",
      "Populating counts for fragment group 300/4084\n",
      "Populating counts for fragment group 400/4084\n",
      "Populating counts for fragment group 500/4084\n",
      "Populating counts for fragment group 600/4084\n",
      "Populating counts for fragment group 700/4084\n",
      "Populating counts for fragment group 800/4084\n",
      "Populating counts for fragment group 900/4084\n",
      "Populating counts for fragment group 1000/4084\n",
      "Populating counts for fragment group 1100/4084\n",
      "Populating counts for fragment group 1200/4084\n",
      "Populating counts for fragment group 1300/4084\n",
      "Populating counts for fragment group 1400/4084\n",
      "Populating counts for fragment group 1500/4084\n",
      "Populating counts for fragment group 1600/4084\n",
      "Populating counts for fragment group 1700/4084\n",
      "Populating counts for fragment group 1800/4084\n",
      "Populating counts for fragment group 1900/4084\n",
      "Populating counts for fragment group 2000/4084\n",
      "Populating counts for fragment group 2100/4084\n",
      "Populating counts for fragment group 2200/4084\n",
      "Populating counts for fragment group 2300/4084\n",
      "Populating counts for fragment group 2400/4084\n",
      "Populating counts for fragment group 2500/4084\n",
      "Populating counts for fragment group 2600/4084\n",
      "Populating counts for fragment group 2700/4084\n",
      "Populating counts for fragment group 2800/4084\n",
      "Populating counts for fragment group 2900/4084\n",
      "Populating counts for fragment group 3000/4084\n",
      "Populating counts for fragment group 3100/4084\n",
      "Populating counts for fragment group 3200/4084\n",
      "Populating counts for fragment group 3300/4084\n",
      "Populating counts for fragment group 3400/4084\n",
      "Populating counts for fragment group 3500/4084\n",
      "Populating counts for fragment group 3600/4084\n",
      "Populating counts for fragment group 3700/4084\n",
      "Populating counts for fragment group 3800/4084\n",
      "Populating counts for fragment group 3900/4084\n",
      "Populating counts for fragment group 4000/4084\n",
      "Populating counts for loss group 0/1682\n",
      "Populating counts for loss group 100/1682\n",
      "Populating counts for loss group 200/1682\n",
      "Populating counts for loss group 300/1682\n",
      "Populating counts for loss group 400/1682\n",
      "Populating counts for loss group 500/1682\n",
      "Populating counts for loss group 600/1682\n",
      "Populating counts for loss group 700/1682\n",
      "Populating counts for loss group 800/1682\n",
      "Populating counts for loss group 900/1682\n",
      "Populating counts for loss group 1000/1682\n",
      "Populating counts for loss group 1100/1682\n",
      "Populating counts for loss group 1200/1682\n",
      "Populating counts for loss group 1300/1682\n",
      "Populating counts for loss group 1400/1682\n",
      "Populating counts for loss group 1500/1682\n",
      "Populating counts for loss group 1600/1682\n",
      "Normalising dataframe 0\n",
      "file 0 normalising\n",
      "file 0 normalised csc shape (810, 5766)\n",
      "Normalising dataframe 1\n",
      "file 1 normalising\n",
      "file 1 normalised csc shape (605, 5766)\n",
      "Normalising dataframe 2\n",
      "file 2 normalising\n",
      "file 2 normalised csc shape (621, 5766)\n",
      "Normalising dataframe 3\n",
      "file 3 normalising\n",
      "file 3 normalised csc shape (731, 5766)\n",
      "Normalising dataframe 4\n",
      "file 4 normalising\n",
      "file 4 normalised csc shape (715, 5766)\n",
      "Normalising dataframe 5\n",
      "file 5 normalising\n",
      "file 5 normalised csc shape (652, 5766)\n",
      "Normalising dataframe 6\n",
      "file 6 normalising\n",
      "file 6 normalised csc shape (558, 5766)\n",
      "Normalising dataframe 7\n",
      "file 7 normalising\n",
      "file 7 normalised csc shape (649, 5766)\n",
      "Normalising dataframe 8\n",
      "file 8 normalising\n",
      "file 8 normalised csc shape (681, 5766)\n",
      "Normalising dataframe 9\n",
      "file 9 normalising\n",
      "file 9 normalised csc shape (807, 5766)\n",
      "Normalising dataframe 10\n",
      "file 10 normalising\n",
      "file 10 normalised csc shape (656, 5766)\n",
      "Normalising dataframe 11\n",
      "file 11 normalising\n",
      "file 11 normalised csc shape (674, 5766)\n",
      "Normalising dataframe 12\n",
      "file 12 normalising\n",
      "file 12 normalised csc shape (667, 5766)\n"
     ]
    }
   ],
   "source": [
    "extractor = extract_features(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                            loss_threshold_min_count, loss_threshold_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (810, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 810\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 7804\n",
      "\n",
      "File 1\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (605, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 605\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6035\n",
      "\n",
      "File 2\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (621, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 621\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 5306\n",
      "\n",
      "File 3\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (731, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 731\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6987\n",
      "\n",
      "File 4\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (715, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 715\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6556\n",
      "\n",
      "File 5\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (652, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 652\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 5909\n",
      "\n",
      "File 6\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (558, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 558\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 4888\n",
      "\n",
      "File 7\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (649, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 649\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 5753\n",
      "\n",
      "File 8\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (681, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 681\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 5920\n",
      "\n",
      "File 9\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (807, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 807\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 7748\n",
      "\n",
      "File 10\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (656, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 656\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 5609\n",
      "\n",
      "File 11\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (674, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 674\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6279\n",
      "\n",
      "File 12\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (667, 5766)\n",
      "Vocab 5766 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 667\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 6580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in range(extractor.F):\n",
    "    mat, vocab, ms1, ms2 = extractor.get_entry(f)\n",
    "    print 'File %d' % f\n",
    "    print 'Count matrix', type(mat), mat.shape\n",
    "    print 'Vocab', len(vocab), 'words'\n",
    "    print 'MS1 rows', type(ms1), ms1.shape[0]\n",
    "    print 'MS2 rows', type(ms2), ms2.shape[0]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_word_index = {}\n",
    "for i,v in enumerate(vocab):\n",
    "    global_word_index[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0\n",
      "810 5766\n",
      "Processing file 1\n",
      "605 5766\n",
      "Processing file 2\n",
      "621 5766\n",
      "Processing file 3\n",
      "731 5766\n",
      "Processing file 4\n",
      "715 5766\n",
      "Processing file 5\n",
      "652 5766\n",
      "Processing file 6\n",
      "558 5766\n",
      "Processing file 7\n",
      "649 5766\n",
      "Processing file 8\n",
      "681 5766\n",
      "Processing file 9\n",
      "807 5766\n",
      "Processing file 10\n",
      "656 5766\n",
      "Processing file 11\n",
      "674 5766\n",
      "Processing file 12\n",
      "667 5766\n"
     ]
    }
   ],
   "source": [
    "corpus_list = []\n",
    "for f in range(extractor.F):\n",
    "    print \"Processing file {}\".format(f)\n",
    "    corpus = {}\n",
    "    mat, vocab, ms1, ms2 = extractor.get_entry(f)\n",
    "    n_docs,n_words = mat.shape\n",
    "    print n_docs,n_words\n",
    "    d_pos = 0\n",
    "    for d in ms1.iterrows():\n",
    "        doc_name = \"{}_{}\".format(d[1]['mz'],d[1]['rt'])\n",
    "        corpus[doc_name] = {}\n",
    "        for word_index,count in zip(mat[d_pos,:].rows[0],mat[d_pos,:].data[0]):\n",
    "            if count > 0:\n",
    "                corpus[doc_name][vocab[word_index]] = count\n",
    "        d_pos += 1\n",
    "    corpus_list.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lda import VariationalLDA,MultiFileVariationalLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object created with 810 documents\n",
      "Object created with 605 documents\n",
      "Object created with 621 documents\n",
      "Object created with 731 documents\n",
      "Object created with 715 documents\n",
      "Object created with 652 documents\n",
      "Object created with 558 documents\n",
      "Object created with 649 documents\n",
      "Object created with 681 documents\n",
      "Object created with 807 documents\n",
      "Object created with 656 documents\n",
      "Object created with 674 documents\n",
      "Object created with 667 documents\n"
     ]
    }
   ],
   "source": [
    "mf_lda = MultiFileVariationalLDA(corpus_list=corpus_list,word_index = global_word_index,K = 300,alpha=1,eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "382.628814432\n",
      "Iteration: 1\n",
      "7.52493054113\n",
      "Iteration: 2\n",
      "5.16881634687\n",
      "Iteration: 3\n",
      "4.22370252736\n",
      "Iteration: 4\n",
      "3.63471692611\n",
      "Iteration: 5\n",
      "3.23560365594\n",
      "Iteration: 6\n",
      "2.9547365027\n",
      "Iteration: 7\n",
      "2.75736819649\n",
      "Iteration: 8\n",
      "2.63233730848\n",
      "Iteration: 9\n"
     ]
    }
   ],
   "source": [
    "mf_lda.run_vb(n_its=500,initialise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lda import MultiFileVariationalLDAPlotter\n",
    "mp = MultiFileVariationalLDAPlotter(mf_lda)\n",
    "mp.multi_alpha(normalise=True,names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2,whiten=True)\n",
    "al = np.zeros((len(corpus_list),mf_lda.K))\n",
    "for i,c in enumerate(corpus_list):\n",
    "    al[i,:] = mf_lda.individual_lda[i].alpha\n",
    "\n",
    "pca.fit(al)\n",
    "X = pca.transform(al)\n",
    "print X.shape\n",
    "\n",
    "import plotly as plotly\n",
    "from plotly.graph_objs import *\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "names = []\n",
    "for m1,m2 in input_set:\n",
    "    names.append(m1.split('/')[-1].split('_')[0])\n",
    "\n",
    "data = []\n",
    "data.append(\n",
    "    Scatter(\n",
    "        x = X[:,0],\n",
    "        y = X[:,1],\n",
    "        mode = 'markers',\n",
    "        text = names,\n",
    "#         marker = dict(\n",
    "#             size = sizes,\n",
    "#         ),\n",
    "    )\n",
    ")\n",
    "\n",
    "plotly.offline.iplot({'data':data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urine_metadata = {}\n",
    "with open('urine_metadata.csv','r') as f:\n",
    "    heads = f.readline()\n",
    "    split_heads = heads.rstrip().split(',')\n",
    "    for line in f:\n",
    "        split_line = line.rstrip().split(',')\n",
    "        sample_id = split_line[0]\n",
    "        urine_metadata[sample_id] = {}\n",
    "        for i,v in enumerate(split_line):\n",
    "            if i == 0 :\n",
    "                continue\n",
    "            if len(v)>0:\n",
    "                urine_metadata[sample_id][split_heads[i]] = float(v)\n",
    "urine_metadata['Pooled'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urine_names_short = []\n",
    "for n in names:\n",
    "    if n.startswith('Urine'):\n",
    "        urine_names_short.append(n[5:])\n",
    "    else:\n",
    "        urine_names_short.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for u in urine_names_short:\n",
    "    sizes.append(50.0+ 50.0 * urine_metadata[u].get('ARB',50.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizes[0] = 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drug = 'Sex F=1'\n",
    "# drug = 'ACE I'\n",
    "drug = 'Any Diabetes'\n",
    "drug = 'Alcohol Excess'\n",
    "drug = 'Diuretic'\n",
    "drug = '_blocker'\n",
    "drug = 'Ca Antag'\n",
    "# find the indices of the files that we have and if they have this drug or not\n",
    "have = []\n",
    "havenot = []\n",
    "for i,u in enumerate(urine_names_short):\n",
    "    status = urine_metadata[u].get(drug,-1)\n",
    "    if status == 1:\n",
    "        have.append(i)\n",
    "    elif status == 0:\n",
    "        havenot.append(i)\n",
    "\n",
    "print have\n",
    "print havenot\n",
    "total_found = len(have) + len(havenot)\n",
    "almat = np.zeros((len(mf_lda.individual_lda),mf_lda.K),np.float)\n",
    "for i,l in enumerate(mf_lda.individual_lda):\n",
    "    almat[i,:] = l.alpha.copy()\n",
    "    \n",
    "have_mean = almat[have,:].mean(axis=0)\n",
    "havenot_mean = almat[havenot,:].mean(axis=0)\n",
    "have_std = almat[have,:].std(axis=0)\n",
    "havenot_std = almat[havenot,:].std(axis=0)\n",
    "score = np.abs(have_mean-havenot_mean)/(have_std + havenot_std)\n",
    "data = []\n",
    "data.append(\n",
    "    Scatter(\n",
    "        x = have_mean,\n",
    "        y = havenot_mean,\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 20*score,\n",
    "            ),\n",
    "    )\n",
    ")\n",
    "plotly.offline.iplot({'data':data})\n",
    "\n",
    "best_topics = zip(range(mf_lda.K),score)\n",
    "best_topics = sorted(best_topics,key=lambda x: x[1],reverse=True)\n",
    "\n",
    "for i in range(5):\n",
    "    best_topic = best_topics[i][0]\n",
    "    data = []\n",
    "    data.append(\n",
    "        Bar(\n",
    "            x = range(len(have) + len(havenot)),\n",
    "            y = almat[have + havenot,best_topic],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plotly.offline.iplot({'data':data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the standard deviation of each topic and display the topics with top 10 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "almat = np.zeros((len(mf_lda.individual_lda),mf_lda.K),np.float)\n",
    "bmat = mf_lda.individual_lda[0].beta_matrix.copy()\n",
    "for i,l in enumerate(mf_lda.individual_lda):\n",
    "    almat[i,:] = l.alpha.copy()\n",
    "topic_std = almat.std(axis=0)\n",
    "best_topics = zip(range(mf_lda.K),topic_std)\n",
    "best_topics = sorted(best_topics,key = lambda x:x[1],reverse=True)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    \n",
    "    best_topic = best_topics[i][0]\n",
    "    this_topic_std = best_topics[i][1]\n",
    "    print \"Topic {}, std {}\".format(best_topic,this_topic_std)\n",
    "    n_al = zip(names,almat[:,best_topic])\n",
    "    n_al = sorted(n_al,key=lambda x:x[1],reverse=True)\n",
    "    for n,a in n_al:\n",
    "        print \"\\t{} alpha = {}\".format(n,a)\n",
    "    print\n",
    "    \n",
    "    data = []\n",
    "    al = almat[:,best_topic]\n",
    "    scale_fac = 50.0/al.max()\n",
    "    data.append(\n",
    "        Scatter(\n",
    "            x = X[:,0],\n",
    "            y = X[:,1],\n",
    "            mode = 'markers',\n",
    "            text = names,\n",
    "            marker = dict(\n",
    "                size = scale_fac*al\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    word_prob = []\n",
    "    for w in mf_lda.individual_lda[0].word_index:\n",
    "        word_prob.append((w,bmat[best_topic,mf_lda.individual_lda[0].word_index[w]]))\n",
    "    word_prob = sorted(word_prob,key = lambda x:x[1],reverse = True)\n",
    "    \n",
    "    cum_prob = 0\n",
    "    for w,p in word_prob[:10]:\n",
    "        cum_prob += p  \n",
    "        print \"\\t\\t{}: {} ({})\".format(w,p,cum_prob)\n",
    "    \n",
    "    \n",
    "    plotly.offline.iplot({'data':data})\n",
    "    \n",
    "    print\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
