{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Extraction for Multifile LDA</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import sys\n",
    "basedir = '/Users/simon/Dropbox/beer_analysis/frans stuff/'\n",
    "sys.path.append(basedir)\n",
    "codedir = '/Users/simon/git/lda/code/'\n",
    "sys.path.append(codedir)\n",
    "\n",
    "from multifile_feature import SparseFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                     loss_threshold_min_count, loss_threshold_max_val):\n",
    "    \n",
    "    extractor = SparseFeatureExtractor(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                                       loss_threshold_min_count, loss_threshold_max_val,\n",
    "                                       input_type='filename')\n",
    "    \n",
    "    # create the grouping for the fragments\n",
    "    fragment_q = extractor.make_fragment_queue()\n",
    "    fragment_groups = extractor.group_features(fragment_q, extractor.fragment_grouping_tol)\n",
    "    \n",
    "    # create the grouping for the losses\n",
    "    loss_q = extractor.make_loss_queue()\n",
    "    loss_groups = extractor.group_features(loss_q, extractor.loss_grouping_tol, \n",
    "                                           check_threshold=True)\n",
    "    \n",
    "    # populate the counts\n",
    "    extractor.create_counts(fragment_groups, loss_groups, scaling_factor)\n",
    "    \n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaling_factor=100             # previously set to 100 in the single file LDA \n",
    "fragment_grouping_tol=7        # grouping tolerance in ppm for the fragment\n",
    "loss_grouping_tol=7            # grouping tolerance in ppm for the neutral loss\n",
    "loss_threshold_min_count=5     # min. counts of loss values to occur\n",
    "loss_threshold_max_val=200     # max. loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_set = [\n",
    "    (basedir+'input/beer/Beer1pos_MS1filter_Method3_ms1.csv', basedir+'input/beer/Beer1pos_MS1filter_Method3_ms2.csv'),\n",
    "    (basedir+'input/beer/Beer2pos_MS1filter_Method3_ms1.csv', basedir+'input/beer/Beer2pos_MS1filter_Method3_ms2.csv'),\n",
    "    (basedir+'input/beer/Beer3pos_MS1filter_Method3_ms1.csv', basedir+'input/beer/Beer3pos_MS1filter_Method3_ms2.csv'),\n",
    "    (basedir+'input/urine/Urine37_pos_ms1.csv', basedir+'input/urine/Urine37_pos_ms2.csv'),\n",
    "    (basedir+'input/urine/Urine44_pos_ms1.csv', basedir+'input/urine/Urine44_pos_ms2.csv'),\n",
    "    (basedir+'input/urine/Urine64_pos_ms1.csv', basedir+'input/urine/Urine64_pos_ms2.csv'),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/beer/Beer1pos_MS1filter_Method3_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/beer/Beer1pos_MS1filter_Method3_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/beer/Beer2pos_MS1filter_Method3_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/beer/Beer2pos_MS1filter_Method3_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/beer/Beer3pos_MS1filter_Method3_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/beer/Beer3pos_MS1filter_Method3_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/urine/Urine37_pos_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/urine/Urine37_pos_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/urine/Urine44_pos_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/urine/Urine44_pos_ms2.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/urine/Urine64_pos_ms1.csv\n",
      "Loading /Users/simon/Dropbox/beer_analysis/frans stuff/input/urine/Urine64_pos_ms2.csv\n",
      "Processing fragments for file 0\n",
      "Processing fragments for file 1\n",
      "Processing fragments for file 2\n",
      "Processing fragments for file 3\n",
      "Processing fragments for file 4\n",
      "Processing fragments for file 5\n",
      "Total groups=10769\n",
      "Processing losses for file 0\n",
      "Processing losses for file 1\n",
      "Processing losses for file 2\n",
      "Processing losses for file 3\n",
      "Processing losses for file 4\n",
      "Processing losses for file 5\n",
      "Total groups=3544\n",
      "Initialising sparse matrix 0\n",
      "Initialising sparse matrix 1\n",
      "Initialising sparse matrix 2\n",
      "Initialising sparse matrix 3\n",
      "Initialising sparse matrix 4\n",
      "Initialising sparse matrix 5\n",
      "Populating the counts\n",
      "Populating counts for fragment group 0/10769\n",
      "Populating counts for fragment group 100/10769\n",
      "Populating counts for fragment group 200/10769\n",
      "Populating counts for fragment group 300/10769\n",
      "Populating counts for fragment group 400/10769\n",
      "Populating counts for fragment group 500/10769\n",
      "Populating counts for fragment group 600/10769\n",
      "Populating counts for fragment group 700/10769\n",
      "Populating counts for fragment group 800/10769\n",
      "Populating counts for fragment group 900/10769\n",
      "Populating counts for fragment group 1000/10769\n",
      "Populating counts for fragment group 1100/10769\n",
      "Populating counts for fragment group 1200/10769\n",
      "Populating counts for fragment group 1300/10769\n",
      "Populating counts for fragment group 1400/10769\n",
      "Populating counts for fragment group 1500/10769\n",
      "Populating counts for fragment group 1600/10769\n",
      "Populating counts for fragment group 1700/10769\n",
      "Populating counts for fragment group 1800/10769\n",
      "Populating counts for fragment group 1900/10769\n",
      "Populating counts for fragment group 2000/10769\n",
      "Populating counts for fragment group 2100/10769\n",
      "Populating counts for fragment group 2200/10769\n",
      "Populating counts for fragment group 2300/10769\n",
      "Populating counts for fragment group 2400/10769\n",
      "Populating counts for fragment group 2500/10769\n",
      "Populating counts for fragment group 2600/10769\n",
      "Populating counts for fragment group 2700/10769\n",
      "Populating counts for fragment group 2800/10769\n",
      "Populating counts for fragment group 2900/10769\n",
      "Populating counts for fragment group 3000/10769\n",
      "Populating counts for fragment group 3100/10769\n",
      "Populating counts for fragment group 3200/10769\n",
      "Populating counts for fragment group 3300/10769\n",
      "Populating counts for fragment group 3400/10769\n",
      "Populating counts for fragment group 3500/10769\n",
      "Populating counts for fragment group 3600/10769\n",
      "Populating counts for fragment group 3700/10769\n",
      "Populating counts for fragment group 3800/10769\n",
      "Populating counts for fragment group 3900/10769\n",
      "Populating counts for fragment group 4000/10769\n",
      "Populating counts for fragment group 4100/10769\n",
      "Populating counts for fragment group 4200/10769\n",
      "Populating counts for fragment group 4300/10769\n",
      "Populating counts for fragment group 4400/10769\n",
      "Populating counts for fragment group 4500/10769\n",
      "Populating counts for fragment group 4600/10769\n",
      "Populating counts for fragment group 4700/10769\n",
      "Populating counts for fragment group 4800/10769\n",
      "Populating counts for fragment group 4900/10769\n",
      "Populating counts for fragment group 5000/10769\n",
      "Populating counts for fragment group 5100/10769\n",
      "Populating counts for fragment group 5200/10769\n",
      "Populating counts for fragment group 5300/10769\n",
      "Populating counts for fragment group 5400/10769\n",
      "Populating counts for fragment group 5500/10769\n",
      "Populating counts for fragment group 5600/10769\n",
      "Populating counts for fragment group 5700/10769\n",
      "Populating counts for fragment group 5800/10769\n",
      "Populating counts for fragment group 5900/10769\n",
      "Populating counts for fragment group 6000/10769\n",
      "Populating counts for fragment group 6100/10769\n",
      "Populating counts for fragment group 6200/10769\n",
      "Populating counts for fragment group 6300/10769\n",
      "Populating counts for fragment group 6400/10769\n",
      "Populating counts for fragment group 6500/10769\n",
      "Populating counts for fragment group 6600/10769\n",
      "Populating counts for fragment group 6700/10769\n",
      "Populating counts for fragment group 6800/10769\n",
      "Populating counts for fragment group 6900/10769\n",
      "Populating counts for fragment group 7000/10769\n",
      "Populating counts for fragment group 7100/10769\n",
      "Populating counts for fragment group 7200/10769\n",
      "Populating counts for fragment group 7300/10769\n",
      "Populating counts for fragment group 7400/10769\n",
      "Populating counts for fragment group 7500/10769\n",
      "Populating counts for fragment group 7600/10769\n",
      "Populating counts for fragment group 7700/10769\n",
      "Populating counts for fragment group 7800/10769\n",
      "Populating counts for fragment group 7900/10769\n",
      "Populating counts for fragment group 8000/10769\n",
      "Populating counts for fragment group 8100/10769\n",
      "Populating counts for fragment group 8200/10769\n",
      "Populating counts for fragment group 8300/10769\n",
      "Populating counts for fragment group 8400/10769\n",
      "Populating counts for fragment group 8500/10769\n",
      "Populating counts for fragment group 8600/10769\n",
      "Populating counts for fragment group 8700/10769\n",
      "Populating counts for fragment group 8800/10769\n",
      "Populating counts for fragment group 8900/10769\n",
      "Populating counts for fragment group 9000/10769\n",
      "Populating counts for fragment group 9100/10769\n",
      "Populating counts for fragment group 9200/10769\n",
      "Populating counts for fragment group 9300/10769\n",
      "Populating counts for fragment group 9400/10769\n",
      "Populating counts for fragment group 9500/10769\n",
      "Populating counts for fragment group 9600/10769\n",
      "Populating counts for fragment group 9700/10769\n",
      "Populating counts for fragment group 9800/10769\n",
      "Populating counts for fragment group 9900/10769\n",
      "Populating counts for fragment group 10000/10769\n",
      "Populating counts for fragment group 10100/10769\n",
      "Populating counts for fragment group 10200/10769\n",
      "Populating counts for fragment group 10300/10769\n",
      "Populating counts for fragment group 10400/10769\n",
      "Populating counts for fragment group 10500/10769\n",
      "Populating counts for fragment group 10600/10769\n",
      "Populating counts for fragment group 10700/10769\n",
      "Populating counts for loss group 0/3544\n",
      "Populating counts for loss group 100/3544\n",
      "Populating counts for loss group 200/3544\n",
      "Populating counts for loss group 300/3544\n",
      "Populating counts for loss group 400/3544\n",
      "Populating counts for loss group 500/3544\n",
      "Populating counts for loss group 600/3544\n",
      "Populating counts for loss group 700/3544\n",
      "Populating counts for loss group 800/3544\n",
      "Populating counts for loss group 900/3544\n",
      "Populating counts for loss group 1000/3544\n",
      "Populating counts for loss group 1100/3544\n",
      "Populating counts for loss group 1200/3544\n",
      "Populating counts for loss group 1300/3544\n",
      "Populating counts for loss group 1400/3544\n",
      "Populating counts for loss group 1500/3544\n",
      "Populating counts for loss group 1600/3544\n",
      "Populating counts for loss group 1700/3544\n",
      "Populating counts for loss group 1800/3544\n",
      "Populating counts for loss group 1900/3544\n",
      "Populating counts for loss group 2000/3544\n",
      "Populating counts for loss group 2100/3544\n",
      "Populating counts for loss group 2200/3544\n",
      "Populating counts for loss group 2300/3544\n",
      "Populating counts for loss group 2400/3544\n",
      "Populating counts for loss group 2500/3544\n",
      "Populating counts for loss group 2600/3544\n",
      "Populating counts for loss group 2700/3544\n",
      "Populating counts for loss group 2800/3544\n",
      "Populating counts for loss group 2900/3544\n",
      "Populating counts for loss group 3000/3544\n",
      "Populating counts for loss group 3100/3544\n",
      "Populating counts for loss group 3200/3544\n",
      "Populating counts for loss group 3300/3544\n",
      "Populating counts for loss group 3400/3544\n",
      "Populating counts for loss group 3500/3544\n",
      "Normalising dataframe 0\n",
      "file 0 normalising\n",
      "file 0 normalised csc shape (1282, 14313)\n",
      "Normalising dataframe 1\n",
      "file 1 normalising\n",
      "file 1 normalised csc shape (1567, 14313)\n",
      "Normalising dataframe 2\n",
      "file 2 normalising\n",
      "file 2 normalised csc shape (1422, 14313)\n",
      "Normalising dataframe 3\n",
      "file 3 normalising\n",
      "file 3 normalised csc shape (2756, 14313)\n",
      "Normalising dataframe 4\n",
      "file 4 normalising\n",
      "file 4 normalised csc shape (1690, 14313)\n",
      "Normalising dataframe 5\n",
      "file 5 normalising\n",
      "file 5 normalised csc shape (1796, 14313)\n"
     ]
    }
   ],
   "source": [
    "extractor = extract_features(input_set, fragment_grouping_tol, loss_grouping_tol, \n",
    "                            loss_threshold_min_count, loss_threshold_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print extractor.F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (1282, 14313)\n",
      "Vocab 14313 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 1282\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 24612\n",
      "\n",
      "File 1\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (1567, 14313)\n",
      "Vocab 14313 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 1567\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 30643\n",
      "\n",
      "File 2\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (1422, 14313)\n",
      "Vocab 14313 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 1422\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 27128\n",
      "\n",
      "File 3\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (2756, 14313)\n",
      "Vocab 14313 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 2756\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 34929\n",
      "\n",
      "File 4\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (1690, 14313)\n",
      "Vocab 14313 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 1690\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 34112\n",
      "\n",
      "File 5\n",
      "Count matrix <class 'scipy.sparse.lil.lil_matrix'> (1796, 14313)\n",
      "Vocab 14313 words\n",
      "MS1 rows <class 'pandas.core.frame.DataFrame'> 1796\n",
      "MS2 rows <class 'pandas.core.frame.DataFrame'> 20171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in range(extractor.F):\n",
    "    mat, vocab, ms1, ms2 = extractor.get_entry(f)\n",
    "    print 'File %d' % f\n",
    "    print 'Count matrix', type(mat), mat.shape\n",
    "    print 'Vocab', len(vocab), 'words'\n",
    "    print 'MS1 rows', type(ms1), ms1.shape[0]\n",
    "    print 'MS2 rows', type(ms2), ms2.shape[0]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_word_index = {}\n",
    "for i,v in enumerate(vocab):\n",
    "    global_word_index[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0\n",
      "1282 14313\n",
      "Processing file 1\n",
      "1567 14313\n",
      "Processing file 2\n",
      "1422 14313\n",
      "Processing file 3\n",
      "2756 14313\n",
      "Processing file 4\n",
      "1690 14313\n",
      "Processing file 5\n",
      "1796 14313\n"
     ]
    }
   ],
   "source": [
    "corpus_list = []\n",
    "for f in range(extractor.F):\n",
    "    print \"Processing file {}\".format(f)\n",
    "    corpus = {}\n",
    "    mat, vocab, ms1, ms2 = extractor.get_entry(f)\n",
    "    n_docs,n_words = mat.shape\n",
    "    print n_docs,n_words\n",
    "    d_pos = 0\n",
    "    for d in ms1.iterrows():\n",
    "        doc_name = \"{}_{}\".format(d[1]['mz'],d[1]['rt'])\n",
    "        corpus[doc_name] = {}\n",
    "        for word_index,count in zip(mat[d_pos,:].rows[0],mat[d_pos,:].data[0]):\n",
    "            if count > 0:\n",
    "                corpus[doc_name][vocab[word_index]] = count\n",
    "        d_pos += 1\n",
    "    corpus_list.append(corpus)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lda import MultiFileVariationalLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mf_lda = MultiFileVariationalLDA(corpus_list=corpus_list,word_index = global_word_index,K = 300,alpha=1,eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mf_lda.run_vb(n_its=500,initialise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('beer_v_urine.lda','w') as f:\n",
    "    pickle.dump(mf_lda,f,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lda_plotters import MultiFileVariationalLDAPlotter\n",
    "mp = MultiFileVariationalLDAPlotter(mf_lda)\n",
    "mp.multi_alpha(normalise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run PCA on the alpha values and then plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2,whiten=True)\n",
    "al = np.zeros((len(corpus_list),mf_lda.K))\n",
    "for i,c in enumerate(corpus_list):\n",
    "    al[i,:] = mf_lda.individual_lda[i].alpha\n",
    "\n",
    "pca.fit(al)\n",
    "X = pca.transform(al)\n",
    "print X.shape\n",
    "\n",
    "import plotly as plotly\n",
    "from plotly.graph_objs import *\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "names = []\n",
    "for m1,m2 in input_set:\n",
    "    names.append(m1.split('/')[-1])\n",
    "\n",
    "data = []\n",
    "r = 'rgb(255,0,0)'\n",
    "g = 'rgb(0,255,0)'\n",
    "data.append(\n",
    "    Scatter(\n",
    "        x = X[:,0],\n",
    "        y = X[:,1],\n",
    "        mode = 'markers',\n",
    "        text = names,\n",
    "        marker = dict(\n",
    "            color = [r,r,r,g,g,g],\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "plotly.offline.iplot({'data':data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "have = [0,1,2]\n",
    "havenot = [3,4,5]\n",
    "# for i,u in enumerate(urine_names_short):\n",
    "#     status = urine_metadata[u].get(drug,-1)\n",
    "#     if status == 1:\n",
    "#         have.append(i)\n",
    "#     elif status == 0:\n",
    "#         havenot.append(i)\n",
    "\n",
    "print have\n",
    "print havenot\n",
    "total_found = len(have) + len(havenot)\n",
    "almat = np.zeros((len(mf_lda.individual_lda),mf_lda.K),np.float)\n",
    "for i,l in enumerate(mf_lda.individual_lda):\n",
    "    almat[i,:] = l.alpha.copy()\n",
    "    \n",
    "have_mean = almat[have,:].mean(axis=0)\n",
    "havenot_mean = almat[havenot,:].mean(axis=0)\n",
    "have_std = almat[have,:].std(axis=0)\n",
    "havenot_std = almat[havenot,:].std(axis=0)\n",
    "score = np.abs(have_mean-havenot_mean)/(have_std + havenot_std)\n",
    "data = []\n",
    "data.append(\n",
    "    Scatter(\n",
    "        x = have_mean,\n",
    "        y = havenot_mean,\n",
    "        mode = 'markers',\n",
    "        text = [str(i) for i in range(mf_lda.K)],\n",
    "        marker = dict(\n",
    "            size = 2*score,\n",
    "            ),\n",
    "    )\n",
    "    \n",
    ")\n",
    "data.append(\n",
    "    Scatter(\n",
    "        x = [0,have_mean.max()],\n",
    "        y = [0,have_mean.max()],\n",
    "    )\n",
    ")\n",
    "\n",
    "layout = Layout(\n",
    "    xaxis = dict(\n",
    "        title = 'Mean alpha value in beer',\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Mean alpha value in urine',\n",
    "    )\n",
    ")\n",
    "plotly.offline.iplot({'data':data,'layout':layout})\n",
    "\n",
    "best_topics = zip(range(mf_lda.K),score)\n",
    "best_topics = sorted(best_topics,key=lambda x: x[1],reverse=True)\n",
    "\n",
    "for i in range(5):\n",
    "    best_topic = best_topics[i][0]\n",
    "    data = []\n",
    "    data.append(\n",
    "        Bar(\n",
    "            x = range(len(have) + len(havenot)),\n",
    "            y = almat[have + havenot,best_topic],\n",
    "        )\n",
    "    )\n",
    "   \n",
    "    plotly.offline.iplot({'data':data})\n",
    "    print best_topics[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "beta_mat = mf_lda.individual_lda[0].beta_matrix.copy()\n",
    "for i in range(10):\n",
    "    word_probs = []\n",
    "    best_topic = best_topics[i][0]\n",
    "    print \"Topic {}, score {}\".format(best_topics[i][0],best_topics[i][1])\n",
    "    for word in mf_lda.word_index:\n",
    "        word_pos = mf_lda.word_index[word]\n",
    "        word_probs.append((word,beta_mat[best_topic,word_pos]))\n",
    "    word_probs = sorted(word_probs,key = lambda x:x[1],reverse=True)\n",
    "    cum_prob = 0.0\n",
    "    for j in range(10):\n",
    "        cum_prob += word_probs[j][1]\n",
    "        print \"\\t{}:{} ({})\".format(word_probs[j][0],word_probs[j][1],cum_prob)\n",
    "    print\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
