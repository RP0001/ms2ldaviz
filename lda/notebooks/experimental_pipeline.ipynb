{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New experimental MS2LDA workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Luigi](https://github.com/spotify/luigi), a Python-based pipeline engine. Also [see the slides here](http://www.slideshare.net/erikbern/luigi-presentation-nyc-data-science).\n",
    "\n",
    "Pros:\n",
    "- Modular\n",
    "- Neat?\n",
    "- Any pipeline we code from scratch is probably going to end up like this eventually, so this perhaps saves some time.\n",
    "- Future-proof --- has a lot of support for big data stuff like hdfs, map-reduce etc, although we can ignore them for now.\n",
    "\n",
    "Cons:\n",
    "- Over-engineered?\n",
    "- Introduces another dependency on the pipeline engine. But unlike celery, this is very easy to install, just *pip install luigi* and that's all.\n",
    "- Error messages not very informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import luigi as lg\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "basedir = '/Users/joewandy/git/lda/code/'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "from multifile_feature import SparseFeatureExtractor\n",
    "from lda import MultiFileVariationalLDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These are what we want from the new pipeline.**\n",
    "\n",
    "1. Extracting spectra from the mzml, mzxml files\n",
    "2. Grouping features and exporting in a format that MS2LDA can deal with\n",
    "3. Running MS2LDA\n",
    "4. Visualising\n",
    "\n",
    "with possible variations on step (1), (2) and maybe even (3) too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Step 1\n",
    "\n",
    "Below we define an example class to load existing CSV files that have been created before from an mzxml/mzml (method 3) pair. However you can imagine that here we provide different implementations to load mztab, MSP-style files, whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExtractSpectra(lg.Task):\n",
    "\n",
    "    datadir = lg.Parameter()\n",
    "    prefix = lg.Parameter()\n",
    "\n",
    "    def run(self):\n",
    "        # we could actually extract the spectra from mzxml, mzml files here\n",
    "        print 'Processing %s and %s' % (datadir, prefix)\n",
    "    \n",
    "    def output(self):        \n",
    "        out_dict = {\n",
    "            'ms1': lg.LocalTarget(self.datadir + self.prefix + '_ms1.csv'), \n",
    "            'ms2': lg.LocalTarget(self.datadir + self.prefix + '_ms2.csv') \n",
    "        }\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Step 2\n",
    "\n",
    "Similarly here we define a class to take the output of the *ExtractSpectra* class above (the dependency is defined in the **requires** method below), performs the grouping by detecting gaps along the groups (defined in the **run** method) and produces the output to a pickled file (defined in the **output** method).\n",
    "\n",
    "It would be easy to provide different implementations here based on other methods of grouping as well.\n",
    "\n",
    "I just copied and pasted .. the code below could be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GroupFeatures(lg.Task):\n",
    "    \n",
    "    scaling_factor = lg.IntParameter(default=1000)\n",
    "    fragment_grouping_tol = lg.IntParameter(default=7)\n",
    "    loss_grouping_tol = lg.IntParameter(default=7)\n",
    "    loss_threshold_min_count = lg.IntParameter(default=5)\n",
    "    loss_threshold_max_val = lg.IntParameter(default=200)\n",
    "    loss_threshold_min_val = lg.IntParameter(default=0)\n",
    "\n",
    "    datadir = lg.Parameter()\n",
    "    prefixes = lg.ListParameter()\n",
    "    \n",
    "    def requires(self):\n",
    "        return [ExtractSpectra(datadir=datadir, prefix=prefix) for prefix in self.prefixes]\n",
    "    \n",
    "    def run(self):\n",
    "\n",
    "        # input_set is a list of tuples of (ms1, ms2)\n",
    "        input_set = []\n",
    "        for out_dict in self.input():\n",
    "            ms1 = out_dict['ms1'].path\n",
    "            ms2 = out_dict['ms2'].path\n",
    "            items = (ms1, ms2)\n",
    "            input_set.append(items)\n",
    "\n",
    "        # performs the grouping here\n",
    "        extractor = SparseFeatureExtractor(input_set, self.fragment_grouping_tol, self.loss_grouping_tol, \n",
    "                                           self.loss_threshold_min_count, self.loss_threshold_max_val,\n",
    "                                           self.loss_threshold_min_val,\n",
    "                                           input_type='filename')\n",
    "\n",
    "        fragment_q = extractor.make_fragment_queue()\n",
    "        fragment_groups = extractor.group_features(fragment_q, extractor.fragment_grouping_tol)\n",
    "\n",
    "        loss_q = extractor.make_loss_queue()\n",
    "        loss_groups = extractor.group_features(loss_q, extractor.loss_grouping_tol, \n",
    "                                               check_threshold=True)\n",
    "\n",
    "        extractor.create_counts(fragment_groups, loss_groups, self.scaling_factor)\n",
    "        mat, vocab, ms1, ms2 = extractor.get_entry(0)\n",
    "            \n",
    "        global_word_index = {}\n",
    "        for i,v in enumerate(vocab):\n",
    "            global_word_index[v] = i\n",
    "            \n",
    "        corpus_dictionary = {}    \n",
    "        for f in range(extractor.F):\n",
    "            print \"Processing file {}\".format(f)\n",
    "            corpus = {}\n",
    "            mat, vocab, ms1, ms2 = extractor.get_entry(f)\n",
    "            n_docs,n_words = mat.shape\n",
    "            print n_docs,n_words\n",
    "            d_pos = 0\n",
    "            for d in ms1.iterrows():\n",
    "                doc_name = \"{}_{}\".format(d[1]['mz'],d[1]['rt'])\n",
    "                corpus[doc_name] = {}\n",
    "                for word_index,count in zip(mat[d_pos,:].rows[0],mat[d_pos,:].data[0]):\n",
    "                    if count > 0:\n",
    "                        corpus[doc_name][vocab[word_index]] = count\n",
    "                d_pos += 1\n",
    "\n",
    "            # Added by Simon\n",
    "            name = input_set[f][0].split('/')[-1].split('ms1')[0][:-1]\n",
    "            corpus_dictionary[name] = corpus\n",
    "            \n",
    "        output_dict = {}\n",
    "        output_dict['global_word_index'] = global_word_index\n",
    "        output_dict['corpus_dictionary'] = corpus_dictionary\n",
    "        with self.output().open('w') as f:\n",
    "            pickle.dump(output_dict, f)            \n",
    "            \n",
    "    def output(self):\n",
    "        return lg.LocalTarget('output_dict.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Step 3\n",
    "\n",
    "Finally here we define a RunLDA task that depends on the output of the grouping class above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RunLDA(lg.Task):\n",
    "\n",
    "    n_its = lg.IntParameter(default=10)\n",
    "    K = lg.IntParameter(default=300)\n",
    "    alpha = lg.FloatParameter(default=1)\n",
    "    eta = lg.FloatParameter(default=0.1)\n",
    "    update_alpha = lg.BoolParameter(default=True)\n",
    "    \n",
    "    datadir = lg.Parameter()\n",
    "    prefixes = lg.ListParameter()    \n",
    "\n",
    "    def requires(self):\n",
    "        return GroupFeatures(datadir=self.datadir, prefixes=self.prefixes)\n",
    "    \n",
    "    def run(self):\n",
    "        with self.input().open('r') as f:\n",
    "            output_dict = pickle.load(f)            \n",
    "        global_word_index = output_dict['global_word_index']\n",
    "        corpus_dictionary = output_dict['corpus_dictionary']\n",
    "        mf_lda = MultiFileVariationalLDA(corpus_dictionary, word_index=global_word_index,\n",
    "                                         K=self.K, alpha=self.alpha, eta=self.eta, \n",
    "                                         update_alpha=self.update_alpha)\n",
    "        mf_lda.run_vb(parallel=False, n_its=self.n_its, initialise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir = '/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/'\n",
    "prefixes = [\n",
    "    'Urine_StrokeDrugs_02_T10_POS',\n",
    "    'Urine_StrokeDrugs_03_T10_POS',\n",
    "    'Urine_StrokeDrugs_08_T10_POS',\n",
    "    'Urine_StrokeDrugs_09_T10_POS',\n",
    "]\n",
    "prefixes_json = json.dumps(prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if RunLDA(n_its=10, K=300, alpha=1, eta=0.1, update_alpha=True, datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefixes=[\"Urine_StrokeDrugs_02_T10_POS\", \"Urine_StrokeDrugs_03_T10_POS\", \"Urine_StrokeDrugs_08_T10_POS\", \"Urine_StrokeDrugs_09_T10_POS\"]) is complete\n",
      "/Users/joewandy/anaconda/lib/python2.7/site-packages/luigi/worker.py:305: UserWarning: Task RunLDA(n_its=10, K=300, alpha=1, eta=0.1, update_alpha=True, datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefixes=[\"Urine_StrokeDrugs_02_T10_POS\", \"Urine_StrokeDrugs_03_T10_POS\", \"Urine_StrokeDrugs_08_T10_POS\", \"Urine_StrokeDrugs_09_T10_POS\"]) without outputs has no custom complete() method\n",
      "  is_complete = task.complete()\n",
      "DEBUG: Checking if GroupFeatures(scaling_factor=1000, fragment_grouping_tol=7, loss_grouping_tol=7, loss_threshold_min_count=5, loss_threshold_max_val=200, loss_threshold_min_val=0, datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefixes=[\"Urine_StrokeDrugs_02_T10_POS\", \"Urine_StrokeDrugs_03_T10_POS\", \"Urine_StrokeDrugs_08_T10_POS\", \"Urine_StrokeDrugs_09_T10_POS\"]) is complete\n",
      "INFO: Informed scheduler that task   RunLDA_300_1__Users_joewandy__2bb33be5da   has status   PENDING\n",
      "DEBUG: Checking if ExtractSpectra(datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefix=Urine_StrokeDrugs_02_T10_POS) is complete\n",
      "DEBUG: Checking if ExtractSpectra(datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefix=Urine_StrokeDrugs_03_T10_POS) is complete\n",
      "DEBUG: Checking if ExtractSpectra(datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefix=Urine_StrokeDrugs_08_T10_POS) is complete\n",
      "DEBUG: Checking if ExtractSpectra(datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefix=Urine_StrokeDrugs_09_T10_POS) is complete\n",
      "INFO: Informed scheduler that task   GroupFeatures__Users_joewandy__7_7_0056f025eb   has status   PENDING\n",
      "INFO: Informed scheduler that task   ExtractSpectra__Users_joewandy__Urine_StrokeDrug_387606b629   has status   DONE\n",
      "INFO: Informed scheduler that task   ExtractSpectra__Users_joewandy__Urine_StrokeDrug_b91accbee3   has status   DONE\n",
      "INFO: Informed scheduler that task   ExtractSpectra__Users_joewandy__Urine_StrokeDrug_0c4355dc83   has status   DONE\n",
      "INFO: Informed scheduler that task   ExtractSpectra__Users_joewandy__Urine_StrokeDrug_cae3ae0711   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 10009] Worker Worker(salt=234252635, workers=1, host=C02JF2MEDKQ5.local, username=joewandy, pid=10009) running   GroupFeatures(scaling_factor=1000, fragment_grouping_tol=7, loss_grouping_tol=7, loss_threshold_min_count=5, loss_threshold_max_val=200, loss_threshold_min_val=0, datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefixes=[\"Urine_StrokeDrugs_02_T10_POS\", \"Urine_StrokeDrugs_03_T10_POS\", \"Urine_StrokeDrugs_08_T10_POS\", \"Urine_StrokeDrugs_09_T10_POS\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_02_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_02_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_03_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_03_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_08_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_08_T10_POS_ms2.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_09_T10_POS_ms1.csv\n",
      "Loading /Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/Urine_StrokeDrugs_09_T10_POS_ms2.csv\n",
      "Processing fragments for file 0\n",
      "Processing fragments for file 1\n",
      "Processing fragments for file 2\n",
      "Processing fragments for file 3\n",
      "Total groups=2415\n",
      "Processing losses for file 0\n",
      "Processing losses for file 1\n",
      "Processing losses for file 2\n",
      "Processing losses for file 3\n",
      "Total groups=780\n",
      "Initialising sparse matrix 0\n",
      "Initialising sparse matrix 1\n",
      "Initialising sparse matrix 2\n",
      "Initialising sparse matrix 3\n",
      "Populating the counts\n",
      "Populating counts for fragment group 0/2415\n",
      "Populating counts for fragment group 100/2415\n",
      "Populating counts for fragment group 200/2415\n",
      "Populating counts for fragment group 300/2415\n",
      "Populating counts for fragment group 400/2415\n",
      "Populating counts for fragment group 500/2415\n",
      "Populating counts for fragment group 600/2415\n",
      "Populating counts for fragment group 700/2415\n",
      "Populating counts for fragment group 800/2415\n",
      "Populating counts for fragment group 900/2415\n",
      "Populating counts for fragment group 1000/2415\n",
      "Populating counts for fragment group 1100/2415\n",
      "Populating counts for fragment group 1200/2415\n",
      "Populating counts for fragment group 1300/2415\n",
      "Populating counts for fragment group 1400/2415\n",
      "Populating counts for fragment group 1500/2415\n",
      "Populating counts for fragment group 1600/2415\n",
      "Populating counts for fragment group 1700/2415\n",
      "Populating counts for fragment group 1800/2415\n",
      "Populating counts for fragment group 1900/2415\n",
      "Populating counts for fragment group 2000/2415\n",
      "Populating counts for fragment group 2100/2415\n",
      "Populating counts for fragment group 2200/2415\n",
      "Populating counts for fragment group 2300/2415\n",
      "Populating counts for fragment group 2400/2415\n",
      "Populating counts for loss group 0/780\n",
      "Populating counts for loss group 100/780\n",
      "Populating counts for loss group 200/780\n",
      "Populating counts for loss group 300/780\n",
      "Populating counts for loss group 400/780\n",
      "Populating counts for loss group 500/780\n",
      "Populating counts for loss group 600/780\n",
      "Populating counts for loss group 700/780\n",
      "Normalising dataframe 0\n",
      "file 0 normalising\n",
      "file 0 normalised csc shape (818, 3195)\n",
      "Normalising dataframe 1\n",
      "file 1 normalising\n",
      "file 1 normalised csc shape (685, 3195)\n",
      "Normalising dataframe 2\n",
      "file 2 normalising\n",
      "file 2 normalised csc shape (740, 3195)\n",
      "Normalising dataframe 3\n",
      "file 3 normalising\n",
      "file 3 normalised csc shape (533, 3195)\n",
      "Processing file 0\n",
      "818 3195\n",
      "Processing file 1\n",
      "685 3195\n",
      "Processing file 2\n",
      "740 3195\n",
      "Processing file 3\n",
      "533 3195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 10009] Worker Worker(salt=234252635, workers=1, host=C02JF2MEDKQ5.local, username=joewandy, pid=10009) done      GroupFeatures(scaling_factor=1000, fragment_grouping_tol=7, loss_grouping_tol=7, loss_threshold_min_count=5, loss_threshold_max_val=200, loss_threshold_min_val=0, datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefixes=[\"Urine_StrokeDrugs_02_T10_POS\", \"Urine_StrokeDrugs_03_T10_POS\", \"Urine_StrokeDrugs_08_T10_POS\", \"Urine_StrokeDrugs_09_T10_POS\"])\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   GroupFeatures__Users_joewandy__7_7_0056f025eb   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 10009] Worker Worker(salt=234252635, workers=1, host=C02JF2MEDKQ5.local, username=joewandy, pid=10009) running   RunLDA(n_its=10, K=300, alpha=1, eta=0.1, update_alpha=True, datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefixes=[\"Urine_StrokeDrugs_02_T10_POS\", \"Urine_StrokeDrugs_03_T10_POS\", \"Urine_StrokeDrugs_08_T10_POS\", \"Urine_StrokeDrugs_09_T10_POS\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object created with 740 documents\n",
      "Object created with 818 documents\n",
      "Object created with 533 documents\n",
      "Object created with 685 documents\n",
      "serial processing\n",
      "Iteration: 0\n",
      "415.20895074\n",
      "Iteration: 1\n",
      "22.3113009506\n",
      "Iteration: 2\n",
      "22.4062464857\n",
      "Iteration: 3\n",
      "22.2457084829\n",
      "Iteration: 4\n",
      "23.6348062139\n",
      "Iteration: 5\n",
      "27.7334569719\n",
      "Iteration: 6\n",
      "36.9445439187\n",
      "Iteration: 7\n",
      "54.9165275244\n",
      "Iteration: 8\n",
      "81.7462496313\n",
      "Iteration: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [pid 10009] Worker Worker(salt=234252635, workers=1, host=C02JF2MEDKQ5.local, username=joewandy, pid=10009) done      RunLDA(n_its=10, K=300, alpha=1, eta=0.1, update_alpha=True, datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefixes=[\"Urine_StrokeDrugs_02_T10_POS\", \"Urine_StrokeDrugs_03_T10_POS\", \"Urine_StrokeDrugs_08_T10_POS\", \"Urine_StrokeDrugs_09_T10_POS\"])\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   RunLDA_300_1__Users_joewandy__2bb33be5da   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=234252635, workers=1, host=C02JF2MEDKQ5.local, username=joewandy, pid=10009) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 6 tasks of which:\n",
      "* 4 present dependencies were encountered:\n",
      "    - 4 ExtractSpectra(datadir=/Users/joewandy/Dropbox/Meta_clustering/MS2LDA/large_study/Urine_mzXML_large_study/method_1/POS/, prefix=Urine_StrokeDrugs_02_T10_POS) ...\n",
      "* 2 ran successfully:\n",
      "    - 1 GroupFeatures(...)\n",
      "    - 1 RunLDA(...)\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing external dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.188543666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.run(['RunLDA', '--workers', '1', '--local-scheduler', '--datadir', datadir, '--prefixes', prefixes_json])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
