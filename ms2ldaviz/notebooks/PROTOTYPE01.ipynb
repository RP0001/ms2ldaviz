{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# All relevant imports for this file are done here. \n",
    "# Run the following in terminal if errors are obtained: set DJANGO_SETTINGS_MODULE=ms2ldaviz.settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import sys\n",
    "basedir = 'C:\\\\Users\\\\rpetr\\\\OneDrive\\\\Desktop\\\\DISS_CODE\\\\ms2ldaviz\\\\ms2ldaviz'\n",
    "sys.path.append(basedir)\n",
    "import django\n",
    "import json\n",
    "django.setup()\n",
    "from basicviz.models import Experiment, Alpha, Mass2MotifInstance, FeatureInstance, Feature, Document, Mass2Motif, DocumentMass2Motif, FeatureMass2MotifInstance\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import csv\n",
    "from scipy.special import polygamma as pg\n",
    "from scipy.special import psi as psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. COMMON VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose experiment ID manually from database. Small number is used for normalisation and e-step below. \n",
    "experiment_id=190 \n",
    "experiment = Experiment.objects.get(id=experiment_id)\n",
    "SMALL_NUMBER = 1e-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. CORPUS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all words/features in the database relevant for the experiment. \n",
    "features = Feature.objects.filter(experiment_id=experiment)\n",
    "experiment_words = []\n",
    "for f in features:\n",
    "     if f.id not in experiment_words: \n",
    "        experiment_words.append(f.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match each word to an index. \n",
    "unique_words = {}\n",
    "index = 0\n",
    "for word in experiment_words:\n",
    "    if word not in unique_words.keys():\n",
    "        unique_words.update({word:index})\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prototype uses a single document. The value is known beforehand. Manual input here. \n",
    "# BELOW COMMENTS FOR REFERENCE:\n",
    "# 269485\teawag_0503.ms\n",
    "# 270314\twashington_0978.ms\n",
    "# 270414\teawag_0758.ms\n",
    "# 271247\teawag_0759.ms\n",
    "# 269323 \teawag_0730.ms\n",
    "experiment_docs=[270414]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each document to a specific ID. \n",
    "unique_docs = {}\n",
    "index = 0 \n",
    "for doc in experiment_docs: \n",
    "    unique_docs.update({doc:index})\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words/features for all documents chosen. The output columns are doc_id, word_id and intensity.\n",
    "feature_instances = FeatureInstance.objects.filter(document_id__in=unique_docs.keys(), feature_id__in=unique_words.keys())\n",
    "doc_word_data = []\n",
    "for f in feature_instances:\n",
    "    doc_word_data.append([unique_docs[int(f.document_id)], unique_words[int(f.feature_id)], f.intensity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the corpus in {doc_id:{word_id:word_count}} form.\n",
    "# Inner dictionary is created first {word:word_count} and then added to doc dictionary key.\n",
    "temp_dict1 = {}\n",
    "temp_dict2 = {}\n",
    "for line in doc_word_data: \n",
    "    doc = line[0]\n",
    "    word = line[1]\n",
    "    count = line[2]\n",
    "    temp_dict1.update({word:count})\n",
    "    if doc not in temp_dict2.keys():\n",
    "        temp_dict2.update({doc:temp_dict1})\n",
    "corpus_dict = temp_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topics for the experiment. Map them to indices. \n",
    "mi = Mass2Motif.objects.filter(experiment=experiment)\n",
    "unique_topics = {}\n",
    "index = 0\n",
    "for m in mi: \n",
    "    unique_topics.update({m.id:index})\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. GET ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Alphas from database. Transform them to topic_count length vector.\n",
    "al = Alpha.objects.filter(mass2motif__experiment=experiment).order_by('mass2motif')\n",
    "alphas = {}\n",
    "for a in al:\n",
    "    alphas.update({unique_topics[a.mass2motif_id]: a.value})\n",
    "n_motif = len(alphas)\n",
    "alpha_vec = np.zeros(n_motif)\n",
    "for pos,val in alphas.items():\n",
    "    alpha_vec[pos] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename for ease. \n",
    "alpha_vector = alpha_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. GET BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Beta values from the database in list form -> topic, word, probability.\n",
    "beta_pre_pivot = []\n",
    "mi = Mass2MotifInstance.objects.filter(mass2motif__experiment=experiment)\n",
    "for m in mi:\n",
    "    beta_pre_pivot.append([unique_topics[m.mass2motif_id], unique_words[m.feature_id], m.probability]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the Beta matrix.\n",
    "output_arr_beta = np.array(beta_pre_pivot)\n",
    "K = len(unique_topics)\n",
    "W = len(unique_words)\n",
    "pivot_table = np.zeros((K, W)).astype('float')\n",
    "i = 0\n",
    "max = len(beta_pre_pivot)\n",
    "while i<max:\n",
    "    pivot_table[int(output_arr_beta[i][0]),int(output_arr_beta[i][1])]=output_arr_beta[i][2]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the beta matrix. Beta is now ready to be used in the E-step.\n",
    "pivot_table_normalised = pivot_table\n",
    "i = 0\n",
    "while i<K: \n",
    "    row = pivot_table_normalised[i, :]\n",
    "    adjusted_row = row + SMALL_NUMBER\n",
    "    normalised_row = adjusted_row / np.sum(adjusted_row)\n",
    "    np.sum(normalised_row)\n",
    "    pivot_table_normalised[i, :] = normalised_row\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. GET ORIGINAL GAMMA(IN NORMALISED FORM THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270414\n",
      "57712\n",
      "270414\n",
      "57510\n",
      "270414\n",
      "57827\n",
      "270414\n",
      "57626\n"
     ]
    }
   ],
   "source": [
    "# Get original normalised Gamma values from experiment dictionary input. \n",
    "theta = DocumentMass2Motif.objects.filter(document_id__in=experiment_docs)\n",
    "output_data_theta = []\n",
    "for t in theta:\n",
    "    print t.document_id\n",
    "    print t.mass2motif_id\n",
    "    output_data_theta.append([unique_docs[int(t.document_id)], unique_topics[int(t.mass2motif_id)], t.probability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. GET ORIGINAL PHI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get feature instances. \n",
    "feature_instance = FeatureInstance.objects.filter(document_id__in=experiment_docs)\n",
    "feature_instance_join = {}\n",
    "for i in feature_instance:\n",
    "    feature_instance_join.update({int(i.id):[int(i.document_id), int(unique_words[i.feature_id])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect docs, words and topic arrays(distributions) to create phi.\n",
    "feature_m2m_instance = FeatureMass2MotifInstance.objects.filter(mass2motif__experiment=experiment)\n",
    "phi_list = []\n",
    "for i in feature_m2m_instance:\n",
    "    if i.featureinstance_id in feature_instance_join.keys():\n",
    "        phi_list.append([feature_instance_join[int(i.featureinstance_id)][0], unique_topics[int(i.mass2motif_id)], feature_instance_join[int(i.featureinstance_id)][1],i.probability])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives the original Phi, which in abstract terms is a 3D matrix -> docs * topics * words.\n",
    "# Convert phi ot numpy array format. \n",
    "phi_original = []\n",
    "for line in phi_list: \n",
    "    phi_original.append([line[0],line[2],line[1],line[3]])\n",
    "phi_original_array = np.array(phi_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. E-STEP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - E-step variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any missing variables for the e-step.\n",
    "corpus = corpus_dict\n",
    "beta_matrix = pivot_table_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Initialise phi matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Phi in the format -> {doc:{word:[topic_array]}}\n",
    "phi_matrix={}\n",
    "for doc in corpus: \n",
    "    d = int(doc)\n",
    "    phi_matrix[d] = {}\n",
    "    for word in corpus[doc]:\n",
    "        w = int(word)\n",
    "        phi_matrix[d][w]=np.zeros(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - initialise gamma matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Gamma matrix. \n",
    "# As the prototype has only 1 document - we get a gamma vector of size K.\n",
    "# doc_total = words/document\n",
    "gamma_matrix=np.zeros((int(len(corpus)),int(K))) #3x500 shape\n",
    "for doc in corpus:\n",
    "    doc_total=0.0\n",
    "    for word in corpus[doc]:\n",
    "        doc_total += corpus[doc][word]\n",
    "    gamma_matrix[int(doc),:] = alpha_vector + 1.0*(doc_total/K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - repeat until convergence loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below has been refactored from lda.py at https://github.com/sdrogers/ms2ldaviz/blob/master/lda/code/lda.py\n",
    "# This is an implementation of Blei's loop as described on http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf, page 1005.\n",
    "test_list = []\n",
    "iterations=1000\n",
    "n_words = int(len(unique_words))\n",
    "temp_beta = np.zeros((K, n_words))\n",
    "current_gamma = np.copy(gamma_matrix)\n",
    "for i in range(iterations):   \n",
    "    prev_gamma = np.copy(current_gamma)\n",
    "    for doc in corpus:\n",
    "        d = int(doc)\n",
    "        doc_dict = corpus[doc]\n",
    "        temp_gamma = np.zeros(K) + alpha_vector\n",
    "        for word in doc_dict:  \n",
    "            w = int(word)\n",
    "            log_phi_matrix = np.log(beta_matrix[:,w]) + psi(gamma_matrix[d,:]).T\n",
    "            log_phi_matrix = np.exp(log_phi_matrix - log_phi_matrix.max())\n",
    "            phi_matrix[d][w] = log_phi_matrix/log_phi_matrix.sum()\n",
    "            temp_gamma += phi_matrix[d][w]*corpus[doc][word]\n",
    "            temp_beta[:,w] += phi_matrix[d][w] * corpus[doc][word]\n",
    "        gamma_matrix[d,:] = temp_gamma\n",
    "        pos = np.where(gamma_matrix[d,:]<SMALL_NUMBER)[0]\n",
    "        gamma_matrix[d,pos] = SMALL_NUMBER\n",
    "    current_gamma = np.copy(gamma_matrix)\n",
    "    gamma_diff = ((current_gamma - prev_gamma)**2).sum()\n",
    "    test_list.append([i, gamma_diff])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. COMPARISON OF GAMMA & PHI (original vs calculated in prototype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalised Gamma (Theta) Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform original gamma into a K-length vector. \n",
    "gamma_vector_original = np.zeros(K) \n",
    "for line in range(len(output_data_theta)):\n",
    "    pos = int(output_data_theta[line][1])\n",
    "    prob = output_data_theta[line][2]\n",
    "    gamma_vector_original[pos] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise original gamma vector. \n",
    "gamma_vector_original += SMALL_NUMBER\n",
    "gamma_vector_original /= np.sum(gamma_vector_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and normalise vector for calculated gamma. \n",
    "gamma_vector_calculated = np.zeros(K) \n",
    "gamma_vector_calculated = np.copy(gamma_matrix[0])\n",
    "gamma_vector_calculated /= np.sum(gamma_vector_calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between gammas: 0.22959803954112878\n"
     ]
    }
   ],
   "source": [
    "# Test how the gamma values compare. Format-> [original_gamma, calculated_gamma, difference]\n",
    "gamma_compare = np.zeros([K,3])\n",
    "for i in range(K):\n",
    "    gamma_compare[i,0] = gamma_vector_original[i]\n",
    "    gamma_compare[i,1] = gamma_vector_calculated[i]\n",
    "    gamma_compare[i,2] = abs(gamma_compare[i,0] - gamma_compare[i,1]) \n",
    "print('Max difference between gammas: ' + str(gamma_compare[:,2].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV export of gamma value comparison. \n",
    "np.savetxt(\"compare_gamma_\"+str(experiment_docs[0])+\".csv\", gamma_compare, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi comparison (for words above min \"noise\" intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the words from de corpus dict that have the highest intensity. \n",
    "materiality_level = 20\n",
    "material_words = []\n",
    "for doc in corpus_dict: \n",
    "    for word in corpus_dict[doc]:\n",
    "        w = corpus_dict[doc][word]\n",
    "        if int(w) > materiality_level:\n",
    "            material_words.append([int(word), w]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create material words dictionary too if needed. \n",
    "material_words_dict = {}\n",
    "for line in material_words:\n",
    "    material_words_dict.update({line[0] : line[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of material words list.\n",
    "material_words_array = np.array(material_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the array descending order (optional). \n",
    "material_words_array = material_words_array[(-material_words_array)[:,1].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-31a21f8a2eb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Choose material words for comparison loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmaterial_words_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaterial_words_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# Choose material words for comparison loop.\n",
    "material_words_list = list(material_words_array[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list for phi comparison, each line having doc/word/topic/original_phi/calculated_phi/difference/word_intensity as columns.\n",
    "phi_comparison = []\n",
    "for line in phi_list: \n",
    "    line_word = int(line[2])\n",
    "    if line_word in material_words_list:\n",
    "        line_doc = unique_docs[line[0]]\n",
    "        line_topic = int(line[1])\n",
    "        line_original_prob = line[3]\n",
    "        line_prob = phi_matrix[line_doc][line_word][line_topic]\n",
    "        phi_diff = abs(line_original_prob-line_prob)\n",
    "        intensity = material_words_dict[line_word]\n",
    "        phi_comparison.append([line_doc, line_word, line_topic, line_original_prob, line_prob, phi_diff, intensity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array from list. \n",
    "phi_comparison_array = np.array(phi_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the maximum difference between phi's. \n",
    "max_phi_diff = list(phi_comparison_array[:,5])\n",
    "max_phi_diff = [float(i) for i in max_phi_diff]\n",
    "max_phi_diff.sort()\n",
    "max_phi_diff = max_phi_diff[-1]\n",
    "print('Max difference between phi\\'s: ' + str(max_phi_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot word intensity on x axis and phi difference on y axis on a scatterplot. \n",
    "phi_comparison_intenphi = np.copy(phi_comparison_array[:,5:])\n",
    "x = phi_comparison_intenphi[:,1]\n",
    "y = phi_comparison_intenphi[:,0]\n",
    "fig = plt.figure()\n",
    "scatterplot = fig.add_subplot(1,1,1)\n",
    "plt.scatter(x,y)\n",
    "fig.set_size_inches(20,10)\n",
    "plt.title(\"INTENSITY PHI_DIFF\")\n",
    "plt.xlabel(\"intensity\")\n",
    "plt.ylabel(\"phi_diff\")\n",
    "plt.show()\n",
    "fig.savefig(str(experiment_docs[0])+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV export of phi comparison. \n",
    "np.savetxt(\"compare_phi_\"+str(experiment_docs[0])+\".csv\", phi_comparison_array, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theta differences -> PROTOTYPE02: is it due to thresholding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the material and immaterial words for each topic to see if theta differences are due to thresholding. \n",
    "topics_tested = np.array(output_data_theta)[:,1]\n",
    "temp =[]  \n",
    "for t in topics_tested: \n",
    "    temp.append(int(t))\n",
    "topics_tested = temp\n",
    "print(\"topic\", \"material\", \"immaterial\")\n",
    "for topic in topics_tested: \n",
    "    topic_tested = topic\n",
    "    count_material = 0\n",
    "    count_immaterial = 0\n",
    "    for line in phi_original: \n",
    "        if line[2] == topic_tested:\n",
    "            if line[1] not in material_words_list:\n",
    "                count_immaterial += 1\n",
    "            else:\n",
    "                count_material += 1\n",
    "    print(topic_tested, count_material, count_immaterial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for topic in gamma_vector_calculated: \n",
    "        if topic>0.05: \n",
    "            print topic\n",
    "            print gamma_vector_calculated.tolist().index(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.74230000e+04, 8.79676857e-02],\n",
       "       [5.77120000e+04, 7.02023679e-01]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_vector_list = []\n",
    "for topic in gamma_vector_calculated: \n",
    "        if topic>0.05: \n",
    "            probability = topic\n",
    "            position = gamma_vector_calculated.tolist().index(topic)\n",
    "            for name, pos in unique_topics.items():\n",
    "                if pos == position:\n",
    "                    print position\n",
    "                    gamma_vector_list.append([name, probability])\n",
    "gamma_vector = np.array(gamma_vector_list)\n",
    "gamma_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.75880630e-104, 1.56862745e-104, 2.04819575e-104, 1.56862745e-104,\n",
       "       2.08845699e-104, 2.10658272e-104, 1.37608627e-003, 2.13982185e-104,\n",
       "       2.11189716e-003, 2.02543919e-104, 1.75881052e-104, 2.19751666e-104,\n",
       "       2.16989820e-104, 1.84631812e-104, 1.84631811e-104, 1.56862745e-104,\n",
       "       1.56862745e-104, 2.06908157e-104, 1.84631825e-104, 1.89917345e-104,\n",
       "       2.37689068e-104, 2.18398058e-104, 1.75938096e-104, 2.18398058e-104,\n",
       "       1.56862745e-104, 1.84631813e-104, 1.97195111e-104, 1.75881912e-104,\n",
       "       1.56862745e-104, 2.33128600e-104, 2.04819575e-104, 4.73571838e-003,\n",
       "       2.77364013e-003, 1.56862745e-104, 1.84631827e-104, 1.75881286e-104,\n",
       "       2.02543919e-104, 2.00683406e-003, 1.93908598e-104, 1.56862745e-104,\n",
       "       1.84631812e-104, 1.24372380e-003, 2.06908157e-104, 2.00029086e-104,\n",
       "       1.93908598e-104, 1.56862745e-104, 2.34359472e-003, 2.31570761e-003,\n",
       "       2.24717263e-104, 2.04819575e-104, 1.56862745e-104, 2.04819575e-104,\n",
       "       2.02543919e-104, 5.40645595e-004, 2.61521034e-104, 2.21056088e-104,\n",
       "       2.13982185e-104, 5.92589714e-004, 1.93908598e-104, 2.73495596e-003,\n",
       "       1.89917345e-104, 1.89952785e-104, 1.89917345e-104, 2.00029086e-104,\n",
       "       1.56862745e-104, 1.84631811e-104, 2.06908157e-104, 2.02543919e-104,\n",
       "       2.12365414e-104, 1.89917345e-104, 1.56862745e-104, 1.49349904e-003,\n",
       "       2.56045255e-104, 1.89917345e-104, 1.89917345e-104, 1.56862745e-104,\n",
       "       2.13982185e-104, 2.94850627e-003, 2.10658272e-104, 5.34306520e-004,\n",
       "       2.10658272e-104, 1.84631828e-104, 1.56862745e-104, 2.02543919e-104,\n",
       "       2.06908157e-104, 2.22315944e-104, 4.15494846e-004, 2.36810551e-104,\n",
       "       2.10658272e-104, 1.84631815e-104, 2.04819575e-104, 2.06908157e-104,\n",
       "       2.51451217e-104, 1.93908598e-104, 1.93908598e-104, 1.97195111e-104,\n",
       "       1.75881430e-104, 7.87311145e-004, 1.75880620e-104, 2.10658272e-104,\n",
       "       1.56862745e-104, 1.97195111e-104, 1.89917345e-104, 2.08845699e-104,\n",
       "       1.97195111e-104, 1.84631813e-104, 6.26102732e-004, 1.89917345e-104,\n",
       "       1.56862745e-104, 1.89917345e-104, 1.56862745e-104, 1.84631829e-104,\n",
       "       2.54116427e-104, 1.93908598e-104, 1.89917345e-104, 1.47208029e-003,\n",
       "       2.10658272e-104, 5.47733727e-004, 1.64381493e-003, 5.77726010e-004,\n",
       "       1.84631812e-104, 1.84631814e-104, 2.10658272e-104, 2.12365414e-104,\n",
       "       2.06908157e-104, 1.97195111e-104, 2.00029086e-104, 1.89917345e-104,\n",
       "       2.20525680e-003, 5.11290056e-004, 1.56862745e-104, 2.00029086e-104,\n",
       "       2.00029086e-104, 1.84631811e-104, 2.50074743e-104, 1.84631814e-104,\n",
       "       5.76514256e-004, 2.04819575e-104, 2.41226296e-003, 1.84631812e-104,\n",
       "       2.06908157e-104, 2.15520467e-104, 1.75952642e-104, 1.75880721e-104,\n",
       "       2.00029086e-104, 1.84631812e-104, 1.89917345e-104, 2.33128600e-104,\n",
       "       1.56862745e-104, 1.84631811e-104, 2.33128600e-104, 2.41058756e-104,\n",
       "       2.86930734e-003, 1.56862745e-104, 2.25865129e-104, 1.93908598e-104,\n",
       "       2.15520467e-104, 2.40236660e-104, 1.89917345e-104, 8.79676857e-002,\n",
       "       1.75882868e-104, 1.56862745e-104, 7.45918144e-004, 1.84631818e-104,\n",
       "       2.08845699e-104, 1.93908598e-104, 2.08845699e-104, 1.75880628e-104,\n",
       "       2.04819575e-104, 1.84631811e-104, 1.89917345e-104, 2.15520467e-104,\n",
       "       1.56862745e-104, 2.30162495e-104, 1.75880638e-104, 2.08845699e-104,\n",
       "       7.27059887e-004, 1.75881165e-104, 2.27520690e-003, 4.96441936e-004,\n",
       "       1.75880636e-104, 1.84631812e-104, 5.01665589e-004, 1.56862745e-104,\n",
       "       1.84631821e-104, 1.89917345e-104, 1.56862745e-104, 2.00029086e-104,\n",
       "       1.84631811e-104, 1.97195111e-104, 1.95570657e-003, 1.89917345e-104,\n",
       "       4.84506052e-004, 1.56862745e-104, 1.93908598e-104, 2.13982185e-104,\n",
       "       4.71519957e-004, 1.56862745e-104, 1.66152924e-004, 1.97195111e-104,\n",
       "       1.82902038e-002, 1.30096860e-003, 2.04819575e-104, 2.02543919e-104,\n",
       "       1.93908598e-104, 2.02543919e-104, 2.00029086e-104, 1.89917345e-104,\n",
       "       1.18959374e-003, 1.97195111e-104, 3.65517284e-004, 2.12365414e-104,\n",
       "       2.02543919e-104, 2.21056088e-104, 2.02543919e-104, 2.10658272e-104,\n",
       "       6.55648773e-004, 1.97195111e-104, 1.56862745e-104, 1.84631811e-104,\n",
       "       2.02543919e-104, 1.97195111e-104, 1.84631813e-104, 1.97195111e-104,\n",
       "       1.02010551e-003, 2.32161317e-104, 2.13982185e-104, 2.33128600e-104,\n",
       "       1.97195111e-104, 1.93908598e-104, 4.69278660e-004, 2.06908157e-104,\n",
       "       1.75884506e-104, 1.97195111e-104, 2.06908157e-104, 3.04970468e-003,\n",
       "       3.86524995e-003, 2.69289396e-104, 1.56862745e-104, 1.93908598e-104,\n",
       "       1.15238152e-003, 1.84631812e-104, 1.97195111e-104, 1.84631813e-104,\n",
       "       1.56862745e-104, 2.15520467e-104, 5.16605705e-003, 2.70781317e-003,\n",
       "       1.89917345e-104, 1.93908598e-104, 1.89917345e-104, 2.04819575e-104,\n",
       "       1.60433526e-003, 2.06908157e-104, 1.89917345e-104, 1.93908598e-104,\n",
       "       1.56862745e-104, 2.46489270e-104, 1.97195111e-104, 2.04819575e-104,\n",
       "       1.84631812e-104, 3.22172560e-003, 8.75478580e-004, 2.02543919e-104,\n",
       "       2.19751702e-104, 2.02543919e-104, 2.08845699e-104, 1.93908598e-104,\n",
       "       2.12365414e-104, 2.06908157e-104, 1.56862745e-104, 7.02023679e-001,\n",
       "       1.75880772e-104, 1.37831083e-003, 1.97195111e-104, 2.59743540e-104,\n",
       "       1.84631812e-104, 2.16989820e-104, 9.41899167e-004, 1.56862745e-104,\n",
       "       2.08845699e-104, 1.56862745e-104, 1.75880639e-104, 1.79024765e-003,\n",
       "       2.57366235e-003, 1.89917345e-104, 2.44990980e-104, 1.89917345e-104,\n",
       "       1.56862745e-104, 1.41334883e-003, 1.93908598e-104, 1.84631818e-104,\n",
       "       1.84633537e-104, 1.84631812e-104, 1.75880698e-104, 3.23401953e-004,\n",
       "       1.89917345e-104, 2.02543919e-104, 2.23798714e-003, 1.75880678e-104,\n",
       "       1.93908598e-104, 1.84631813e-104, 1.75889220e-104, 1.75883323e-104,\n",
       "       1.56862745e-104, 1.56862745e-104, 1.56862745e-104, 1.56862745e-104,\n",
       "       1.84631812e-104, 2.00029086e-104, 1.89917345e-104, 2.10658272e-104,\n",
       "       1.56862745e-104, 1.84631811e-104, 2.02543919e-104, 3.64220402e-004,\n",
       "       9.89678775e-004, 1.89917345e-104, 2.12365414e-104, 2.00029086e-104,\n",
       "       1.97195111e-104, 2.30162495e-104, 1.82026052e-004, 9.82412869e-004,\n",
       "       1.75880702e-104, 1.96472237e-003, 1.93908598e-104, 2.02543919e-104,\n",
       "       2.00029086e-104, 2.02543919e-104, 1.15302826e-003, 1.56862745e-104,\n",
       "       5.30978982e-003, 1.93908598e-104, 7.80962881e-004, 1.84631812e-104,\n",
       "       1.93908598e-104, 1.63944054e-003, 2.02543919e-104, 9.71205883e-004,\n",
       "       1.97195111e-104, 1.84631830e-104, 2.06908157e-104, 1.84631812e-104,\n",
       "       1.93908598e-104, 1.93908598e-104, 2.25865129e-104, 1.89917345e-104,\n",
       "       1.84631812e-104, 2.13982185e-104, 1.75880671e-104, 1.97195111e-104,\n",
       "       1.97195111e-104, 1.56862745e-104, 2.88132063e-004, 1.56862745e-104,\n",
       "       1.93908598e-104, 6.09807102e-003, 1.29091909e-003, 2.02543919e-104,\n",
       "       1.75881606e-104, 2.45729577e-003, 2.10658272e-104, 1.56862745e-104,\n",
       "       2.06908157e-104, 1.75890612e-104, 2.35916128e-104, 1.54052397e-003,\n",
       "       2.08845699e-104, 2.08845699e-104, 1.56862745e-104, 1.33166215e-003,\n",
       "       3.37242670e-104, 2.19751666e-104, 4.52566068e-004, 1.97195111e-104,\n",
       "       1.18964278e-003, 2.12365414e-104, 2.24717263e-104, 3.37478079e-004,\n",
       "       1.75880987e-104, 1.89917345e-104, 2.19751666e-104, 1.56862745e-104,\n",
       "       6.29542013e-004, 1.89917345e-104, 2.02543919e-104, 1.84631811e-104,\n",
       "       1.93908598e-104, 2.16989820e-104, 1.75889016e-104, 3.11179622e-003,\n",
       "       1.25748493e-003, 2.04819575e-104, 1.89917345e-104, 7.47769222e-004,\n",
       "       2.18398058e-104, 2.00029086e-104, 1.36799838e-003, 1.89917345e-104,\n",
       "       1.89917345e-104, 2.15520467e-104, 2.82887544e-104, 1.56862745e-104,\n",
       "       2.04819575e-104, 1.97195111e-104, 2.16989820e-104, 1.84631811e-104,\n",
       "       2.10658272e-104, 2.06908157e-104, 2.08845699e-104, 2.80610022e-104,\n",
       "       2.37689068e-104, 4.18791995e-002, 3.09274979e-104, 2.35004955e-104,\n",
       "       3.05356618e-104, 2.71372171e-104, 2.32161317e-104, 2.02543919e-104,\n",
       "       1.75880770e-104, 2.39401432e-104, 2.04819575e-104, 1.93908598e-104,\n",
       "       1.75880665e-104, 2.29128153e-104, 2.05244971e-003, 2.35004955e-104,\n",
       "       3.05356618e-104, 2.88137111e-104, 2.88561901e-104, 7.10926952e-004,\n",
       "       3.09584768e-004, 2.80147572e-104, 1.01866405e-003, 7.17241869e-004,\n",
       "       1.75880650e-104, 2.24717263e-104, 2.02543919e-104, 2.34076113e-104,\n",
       "       2.04819575e-104, 1.56862745e-104, 1.64656490e-004, 2.04819575e-104,\n",
       "       2.41868292e-104, 1.93908598e-104, 3.99142038e-004, 2.16989820e-104,\n",
       "       2.40236660e-104, 1.93908598e-104, 1.56862745e-104, 1.56862745e-104,\n",
       "       1.89918375e-104, 2.24717263e-104, 1.56862745e-104, 2.02543919e-104,\n",
       "       1.47102996e-003, 2.55408602e-104, 2.36810551e-104, 2.06908157e-104,\n",
       "       1.93908598e-104, 1.56862745e-104, 1.75880742e-104, 1.93908598e-104,\n",
       "       1.97195111e-104, 1.93908598e-104, 3.12046416e-104, 2.80610022e-104,\n",
       "       1.01929065e-002, 2.44226695e-104, 2.52128144e-104, 2.50655526e-003,\n",
       "       2.39401432e-104, 2.95521417e-104, 2.25865129e-104, 3.22561455e-104,\n",
       "       2.49167418e-004, 2.43451780e-104, 2.33128600e-104, 1.33166215e-003,\n",
       "       2.53460535e-104, 2.63824360e-104, 2.00029086e-104, 2.35004955e-104,\n",
       "       1.84631812e-104, 1.03809793e-003, 2.38552456e-104, 2.08845699e-104,\n",
       "       2.00029086e-104, 2.16989820e-104, 2.13982185e-104, 2.13982185e-104,\n",
       "       2.13982185e-104, 9.67770037e-004, 2.16989820e-104, 2.29128153e-104,\n",
       "       2.39401432e-104, 2.52128144e-104, 2.81983342e-104, 2.74751267e-003])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_vector_calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 271, 0.925464030966764],\n",
       " [0, 413, 0.0418494260642831],\n",
       " [0, 468, 0.0101604220097222],\n",
       " [0, 200, 0.0159164772206027]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-b82b891311e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgamma_compare\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "gamma_compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
